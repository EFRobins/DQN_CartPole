{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import wrappers\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.learning_rate = 0.995\n",
    "        self.epsilon = 0.35 \n",
    "        self.decay_factor = 0.9975\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.W1 = np.random.randn(hidden_size, input_size) * np.sqrt(1/input_size)\n",
    "        self.b1 = np.zeros((hidden_size, ))\n",
    "        self.W2 = np.random.randn(output_size, hidden_size) * np.sqrt(1/output_size)\n",
    "        self.b2 = np.zeros((output_size, ))\n",
    "\n",
    "        self.replay_buffer = deque(maxlen=1000)\n",
    "\n",
    "    def forward(self, observation):\n",
    "        self.hidden_in = np.dot(self.W1, observation) + self.b1\n",
    "        self.hidden_out = self.relu(self.hidden_in)\n",
    "        self.output_in = np.dot(self.W2, self.hidden_out) + self.b2        \n",
    "        self.output_out = self.sigmoid(self.output_in)\n",
    "        return self.output_out\n",
    "    \n",
    "    def backprop(self, observation, Error):\n",
    "        \"\"\"\n",
    "        # Backpropagation\n",
    "        reluOutput = self.sigmoid_backward(self.output_in)\n",
    "        #print(\"Shape of ReluOutput: \", reluOutput.shape)\n",
    "        delta2 = reluOutput * Error \n",
    "        #print(\"Shape of delta 2:\", delta2.shape)\n",
    "        dw2 = np.outer(delta2, self.hidden_out)\n",
    "        #print(\"Shape of dw2:\", dw2.shape)\n",
    "        db2 = delta2\n",
    "        #print(\"shape of db2:\", db2.shape)\n",
    "        # Backpropagation: hidden layer -> input layer\n",
    "        reluInput = self.relu_backward(self.hidden_in)\n",
    "        #print(\"Shape of ReluInput: \", reluInput.shape)\n",
    "        delta1 = reluInput * np.dot(self.W2.T, delta2)\n",
    "        #print(\"Shape of delta 1:\", delta1.shape)\n",
    "        dw1 = np.outer(delta1, observation)\n",
    "        #print(\"Shape of dw1:\", dw1.shape)\n",
    "        db1 = delta1\n",
    "        #print(\"Shape of db1:\", db1.shape)\"\"\"\n",
    "       \n",
    "        reluOutput = self.sigmoid_backward(self.output_in)    \n",
    "        delta2 = reluOutput * Error\n",
    "\n",
    "        dw2 = np.outer(delta2, self.hidden_out)\n",
    "        db2 = delta2\n",
    "\n",
    "        reluInput = self.relu_backward(self.hidden_in)\n",
    "        delta1 = reluInput * np.dot(self.W2.T, delta2)\n",
    "\n",
    "        dw1 = np.outer(delta1, observation)\n",
    "        db1 = delta1\n",
    "\n",
    "        # Update Rules\n",
    "        self.W1 += self.learning_rate * dw1\n",
    "        self.W2 += self.learning_rate * dw2\n",
    "        self.b1 += self.learning_rate * db1\n",
    "        self.b2 += self.learning_rate * db2  \n",
    "\n",
    "    def store_experience(self, state, Error):\n",
    "        experience = (state, Error)\n",
    "        self.replay_buffer.append(experience)\n",
    "    \n",
    "    def sample_experiences(self):\n",
    "        return random.sample(self.replay_buffer, 10)\n",
    "    \n",
    "    def update_net(self):\n",
    "        sample = random.sample(self.replay_buffer, 10)\n",
    "        for (state, Error) in sample:\n",
    "            self.backprop(state, Error)\n",
    "\n",
    "    def ep_greedy(self, Q_values):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, 1)\n",
    "        else: \n",
    "            return np.argmax(Q_values)\n",
    "        \n",
    "    def decay(self):\n",
    "        self.epsilon *= self.decay_factor\n",
    "\n",
    "    def softmax(self, Z):\n",
    "        e_Z = np.exp(Z - np.max(Z, axis=0))\n",
    "        return e_Z / np.sum(e_Z, axis=0)\n",
    "    \n",
    "    def sigmoid(self, Z):\n",
    "        return 1/(1+np.exp(-Z))\n",
    "\n",
    "    def relu(self, Z):\n",
    "        return np.maximum(0,Z)\n",
    "\n",
    "    def sigmoid_backward(self, layer):\n",
    "        sig = self.sigmoid(layer)\n",
    "        return sig * (1 - sig)\n",
    "\n",
    "    def relu_backward(self, layer):\n",
    "        return np.where(layer > 0, 1, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.85  \n",
    "env = gym.make('CartPole-v1', render_mode=\"human\")\n",
    "observation = env.reset()\n",
    "\n",
    "s_size = env.observation_space.shape[0]\n",
    "a_size = env.action_space.n\n",
    "hidden_size = 8\n",
    "net = ANN(s_size, hidden_size, a_size)\n",
    "\n",
    "for i in range(1, 10000):\n",
    "    done = False\n",
    "    observation = env.reset()\n",
    "    reward_avg = 0\n",
    "    observation = observation[0]\n",
    "    env.render()\n",
    "\n",
    "    while not done:\n",
    "        net.decay()\n",
    "\n",
    "        # Forward prop\n",
    "        Q_values = net.forward(observation)\n",
    "        \n",
    "        # Policy Decision\n",
    "        Q_current = np.max(Q_values)\n",
    "        action = net.ep_greedy(Q_values)\n",
    "        \n",
    "        # Next step\n",
    "        observation_next, reward, done, info, _ = env.step(action)\n",
    "        reward_avg += reward\n",
    "        \n",
    "        if not done: \n",
    "            Error = reward - Q_current\n",
    "            net.store_experience(observation, Error)\n",
    "        else:\n",
    "            Q_values_next = net.forward(observation_next)\n",
    "            Q_max = np.max(Q_values_next)\n",
    "            Q_target = reward + gamma * Q_max \n",
    "            Error = Q_target - Q_current \n",
    "            net.store_experience(observation, Error)\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            net.update_net()\n",
    "            print(reward_avg / 100)\n",
    "            reward_avg = 0\n",
    "        \n",
    "        observation = observation_next\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CartPole-v0\n",
      "CartPole-v1\n",
      "MountainCar-v0\n",
      "MountainCarContinuous-v0\n",
      "Pendulum-v1\n",
      "Acrobot-v1\n",
      "LunarLander-v2\n",
      "LunarLanderContinuous-v2\n",
      "BipedalWalker-v3\n",
      "BipedalWalkerHardcore-v3\n",
      "CarRacing-v2\n",
      "Blackjack-v1\n",
      "FrozenLake-v1\n",
      "FrozenLake8x8-v1\n",
      "CliffWalking-v0\n",
      "Taxi-v3\n",
      "Reacher-v2\n",
      "Reacher-v4\n",
      "Pusher-v2\n",
      "Pusher-v4\n",
      "InvertedPendulum-v2\n",
      "InvertedPendulum-v4\n",
      "InvertedDoublePendulum-v2\n",
      "InvertedDoublePendulum-v4\n",
      "HalfCheetah-v2\n",
      "HalfCheetah-v3\n",
      "HalfCheetah-v4\n",
      "Hopper-v2\n",
      "Hopper-v3\n",
      "Hopper-v4\n",
      "Swimmer-v2\n",
      "Swimmer-v3\n",
      "Swimmer-v4\n",
      "Walker2d-v2\n",
      "Walker2d-v3\n",
      "Walker2d-v4\n",
      "Ant-v2\n",
      "Ant-v3\n",
      "Ant-v4\n",
      "Humanoid-v2\n",
      "Humanoid-v3\n",
      "Humanoid-v4\n",
      "HumanoidStandup-v2\n",
      "HumanoidStandup-v4\n"
     ]
    }
   ],
   "source": [
    "for i in gym.envs.registry:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 1.3648033e-03,  1.4137057e+00,  1.3822515e-01,  1.2380719e-01,\n",
      "       -1.5746783e-03, -3.1310089e-02,  0.0000000e+00,  0.0000000e+00],\n",
      "      dtype=float32), {})\n",
      "Discrete(4)\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "env = gym.make('LunarLander-v2', render_mode=\"human\")\n",
    "observation = env.reset()\n",
    "print(observation)\n",
    "print(env.action_space)\n",
    "print(env.action_space.sample())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
