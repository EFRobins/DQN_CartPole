{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import wrappers\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (159489049.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[11], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    #print(i)\u001b[0m\n\u001b[1;37m             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "for i in gym.envs.registry:\n",
    "    #print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_Net:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.learning_rate = 0.995\n",
    "        self.epsilon = 0.45 \n",
    "        self.decay_factor = 0.95\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.W1 = np.random.randn(hidden_size, input_size) * np.sqrt(1/input_size)\n",
    "        self.b1 = np.zeros((hidden_size, ))\n",
    "        self.W2 = np.random.randn(output_size, hidden_size) * np.sqrt(1/output_size)\n",
    "        self.b2 = np.zeros((output_size, ))\n",
    "    \n",
    "    def forward(self, state):\n",
    "        self.hidden_in = np.dot(self.W1, state) + self.b1\n",
    "        self.hidden_out = self.relu(self.hidden_in)\n",
    "        self.output_in = np.dot(self.W2, self.hidden_out) + self.b2        \n",
    "        self.output_out = self.sigmoid(self.output_in)\n",
    "        return self.output_out\n",
    "    \n",
    "    def backprop(self, observation, Error):\n",
    "        # Backpropagation\n",
    "        reluOutput = self.sigmoid_backward(self.output_in)\n",
    "        #print(\"Shape of ReluOutput: \", reluOutput.shape)\n",
    "\n",
    "        delta2 = reluOutput * Error \n",
    "        #print(\"Shape of delta 2:\", delta2.shape)\n",
    "\n",
    "        dw2 = np.outer(delta2, self.hidden_out)\n",
    "        #print(\"Shape of dw2:\", dw2.shape)\n",
    "\n",
    "        db2 = delta2\n",
    "        #print(\"shape of db2:\", db2.shape)\n",
    "\n",
    "        # Backpropagation: hidden layer -> input layer\n",
    "        reluInput = self.relu_backward(self.hidden_in)\n",
    "        #print(\"Shape of ReluInput: \", reluInput.shape)\n",
    "\n",
    "        delta1 = reluInput * np.dot(self.W2.T, delta2)\n",
    "        #print(\"Shape of delta 1:\", delta1.shape)\n",
    "\n",
    "        dw1 = np.outer(delta1, observation)\n",
    "        #print(\"Shape of dw1:\", dw1.shape)\n",
    "\n",
    "        db1 = delta1\n",
    "        #print(\"Shape of db1:\", db1.shape)\n",
    "       \n",
    "        # Update Rules\n",
    "        self.W1 += self.learning_rate * dw1\n",
    "        self.W2 += self.learning_rate * dw2\n",
    "        self.b1 += self.learning_rate * db1\n",
    "        self.b2 += self.learning_rate * db2  \n",
    "        \n",
    "    def ep_greedy(self, Q_values):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, 1)\n",
    "        else: \n",
    "            return np.argmax(Q_values)\n",
    "        \n",
    "    def decay(self):\n",
    "        self.epsilon *= self.decay_factor\n",
    "\n",
    "    def softmax(self, Z):\n",
    "        e_Z = np.exp(Z - np.max(Z, axis=0))\n",
    "        return e_Z / np.sum(e_Z, axis=0)\n",
    "    \n",
    "    def sigmoid(self, Z):\n",
    "        return 1/(1+np.exp(-Z))\n",
    "\n",
    "    def relu(self, Z):\n",
    "        return np.maximum(0,Z)\n",
    "\n",
    "    def sigmoid_backward(self, layer):\n",
    "        sig = self.sigmoid(layer)\n",
    "        return sig * (1 - sig)\n",
    "\n",
    "    def relu_backward(self, layer):\n",
    "        return np.where(layer > 0, 1, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17\n",
      "41.8\n",
      "37.36\n",
      "40.64\n",
      "38.23\n",
      "39.6\n",
      "38.97\n",
      "37.48\n",
      "41.58\n",
      "44.46\n",
      "44.6\n",
      "39.97\n",
      "38.78\n",
      "38.05\n",
      "44.76\n",
      "41.15\n",
      "45.85\n",
      "42.4\n",
      "46.01\n",
      "42.92\n",
      "44.11\n",
      "44.37\n",
      "42.69\n",
      "43.1\n",
      "40.38\n",
      "44.69\n",
      "48.02\n",
      "42.85\n",
      "41.59\n",
      "42.96\n",
      "47.69\n",
      "46.03\n",
      "47.82\n",
      "42.46\n",
      "44.4\n",
      "45.99\n",
      "42.5\n",
      "42.1\n",
      "47.1\n",
      "45.96\n",
      "42.79\n",
      "40.95\n",
      "44.31\n",
      "42.29\n",
      "47.36\n",
      "48.83\n",
      "46.16\n",
      "44.39\n",
      "46.87\n",
      "43.74\n",
      "49.16\n",
      "45.62\n",
      "44.23\n",
      "39.26\n",
      "42.19\n",
      "46.12\n",
      "40.02\n",
      "48.6\n",
      "50.18\n",
      "47.16\n",
      "41.69\n",
      "47.59\n",
      "45.59\n",
      "45.96\n",
      "54.11\n",
      "50.91\n",
      "47.32\n",
      "41.24\n",
      "48.12\n",
      "45.64\n",
      "49.93\n",
      "48.91\n",
      "45.76\n",
      "43.26\n",
      "45.8\n",
      "47.61\n",
      "49.29\n",
      "49.05\n",
      "49.84\n",
      "42.73\n",
      "43.44\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m action \u001b[39m=\u001b[39m net\u001b[39m.\u001b[39mep_greedy(Q_values)\n\u001b[0;32m     29\u001b[0m \u001b[39m# Next step\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m observation_next, reward, done, info, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     31\u001b[0m reward_avg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m reward\n\u001b[0;32m     33\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m done: \n",
      "File \u001b[1;32m~\\Documents\\OpenGym\\gym\\gym\\wrappers\\time_limit.py:50\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m     40\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \n\u001b[0;32m     42\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     48\u001b[0m \n\u001b[0;32m     49\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m     observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     51\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     53\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[1;32m~\\Documents\\OpenGym\\gym\\gym\\wrappers\\order_enforcing.py:37\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset:\n\u001b[0;32m     36\u001b[0m     \u001b[39mraise\u001b[39;00m ResetNeeded(\u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling env.reset()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[1;32m~\\Documents\\OpenGym\\gym\\gym\\wrappers\\env_checker.py:39\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[39mreturn\u001b[39;00m env_step_passive_checker(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv, action)\n\u001b[0;32m     38\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 39\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[1;32m~\\Documents\\OpenGym\\gym\\gym\\envs\\classic_control\\cartpole.py:187\u001b[0m, in \u001b[0;36mCartPoleEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    184\u001b[0m     reward \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[0;32m    186\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 187\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrender()\n\u001b[0;32m    188\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39marray(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat32), reward, terminated, \u001b[39mFalse\u001b[39;00m, {}\n",
      "File \u001b[1;32m~\\Documents\\OpenGym\\gym\\gym\\envs\\classic_control\\cartpole.py:260\u001b[0m, in \u001b[0;36mCartPoleEnv.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    258\u001b[0m cart_coords \u001b[39m=\u001b[39m [(c[\u001b[39m0\u001b[39m] \u001b[39m+\u001b[39m cartx, c[\u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m carty) \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m cart_coords]\n\u001b[0;32m    259\u001b[0m gfxdraw\u001b[39m.\u001b[39maapolygon(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msurf, cart_coords, (\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m))\n\u001b[1;32m--> 260\u001b[0m gfxdraw\u001b[39m.\u001b[39;49mfilled_polygon(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msurf, cart_coords, (\u001b[39m0\u001b[39;49m, \u001b[39m0\u001b[39;49m, \u001b[39m0\u001b[39;49m))\n\u001b[0;32m    262\u001b[0m l, r, t, b \u001b[39m=\u001b[39m (\n\u001b[0;32m    263\u001b[0m     \u001b[39m-\u001b[39mpolewidth \u001b[39m/\u001b[39m \u001b[39m2\u001b[39m,\n\u001b[0;32m    264\u001b[0m     polewidth \u001b[39m/\u001b[39m \u001b[39m2\u001b[39m,\n\u001b[0;32m    265\u001b[0m     polelen \u001b[39m-\u001b[39m polewidth \u001b[39m/\u001b[39m \u001b[39m2\u001b[39m,\n\u001b[0;32m    266\u001b[0m     \u001b[39m-\u001b[39mpolewidth \u001b[39m/\u001b[39m \u001b[39m2\u001b[39m,\n\u001b[0;32m    267\u001b[0m )\n\u001b[0;32m    269\u001b[0m pole_coords \u001b[39m=\u001b[39m []\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "gamma = 0.85  \n",
    "\n",
    "env = gym.make('CartPole-v1', render_mode=\"human\")\n",
    "state = env.reset()\n",
    "\n",
    "s_size = env.observation_space.shape[0]\n",
    "a_size = env.action_space.n\n",
    "hidden_size = 16\n",
    "\n",
    "net = Q_Net(s_size, hidden_size, a_size)\n",
    "\n",
    "reward_avg = 0\n",
    "\n",
    "for i in range(10000):\n",
    "    done = False\n",
    "    state = env.reset()[0]\n",
    "\n",
    "    while not done:\n",
    "        \n",
    "        net.decay()\n",
    "\n",
    "        # Forward prop\n",
    "        Q_values = net.forward(state)\n",
    "        \n",
    "        # Policy Decision\n",
    "        Q_current = np.max(Q_values)\n",
    "        action = net.ep_greedy(Q_values)\n",
    "        \n",
    "        # Next step\n",
    "        observation_next, reward, done, info, _ = env.step(action)\n",
    "        reward_avg += reward\n",
    "\n",
    "        if not done: \n",
    "            Error = reward - Q_current\n",
    "            net.backprop(state, Error)\n",
    "        else:\n",
    "            Q_values_next = net.forward(observation_next)\n",
    "            Q_max = np.max(Q_values_next)\n",
    "            Q_target = reward + gamma * Q_max \n",
    "            Error = Q_target - Q_current \n",
    "            net.backprop(state, Error)\n",
    "            if i % 100 == 0:\n",
    "                print(reward_avg/100)\n",
    "                reward_avg = 0\n",
    "        \n",
    "        state = observation_next\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
